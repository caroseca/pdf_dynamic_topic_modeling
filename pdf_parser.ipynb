{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse PDFs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "https://github.com/chrismattmann/tika-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required for parsing pdf. Make sure you have java installed  \n",
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from tika import parser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw, raw_parser):\n",
    "    \"\"\"\n",
    "    Takes in a string, lowercases all the text,\n",
    "    fixes hyphenation, then tokenizes the input. \n",
    "    Outputs a list of all the tokens in the input document. \n",
    "    Each token is at least 3 chars long. \n",
    "    Also removed numbers, and stopwords. \n",
    "    Stopwords downloaded from nltk.downloads('stopwords'). \n",
    "    Can be set to read in raw PDFs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first : string, or parser \n",
    "        `raw` is the pdf document\n",
    "    second : bool\n",
    "        True: read `raw` as parser file\n",
    "        False: read `raw` as string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of each token, stopword filtered\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if raw_parser:\n",
    "        lowered = raw['content'].lower()\n",
    "    else:\n",
    "        lowered = raw.lower()\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    additional_list = ['also', 'red', 'cross', 'international', 'federation', 'ndr']\n",
    "    whitespace = re.sub(r'[\\W]+', ' ', lowered)\n",
    "    fixed = re.sub(r'[\\-]\\W+', '', whitespace)\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'[A-z]{3,}')\n",
    "    tokenized = tokenizer.tokenize(fixed)\n",
    "    tokenized = [word for word in tokenized if word not in additional_list]\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in tokenized if word not in stopwords.words('english')]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(tokenized, pdf_name, output_list):\n",
    "    \"\"\"\n",
    "    Appends a dictionary of word, count, and source\n",
    "    corpus for each unique word into output_list. \n",
    "    Returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first : list\n",
    "        `tokenized` is a list of tokens\n",
    "        from a document\n",
    "    second : string\n",
    "        `pdf_name` is the name of the\n",
    "        source PDF the word is from\n",
    "    third : list\n",
    "        `output_list` is the list of \n",
    "        dicts, with the top most common words\n",
    "        appearing in the doc, formatted like \n",
    "        the following: \n",
    "            {\"word\": string, \"count\": int, \"source\": string}\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Nothing\n",
    "\n",
    "    \"\"\"\n",
    "    filtered_count = Counter(tokenized)\n",
    "    for i in filtered_count.most_common():\n",
    "        output_list.append({\"word\": i[0], \"count\": i[1], \"source\": pdf_name})\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_year(item):\n",
    "    \"\"\"\n",
    "    Parses the year from item.\n",
    "    If a regex Match object is found, parses\n",
    "    the Match object for years resembling either\n",
    "    \"20[0-9]{2}\" or \"19[0-9]{2}\"\n",
    "    \n",
    "    Returns the int of the parsed year. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first : string\n",
    "        `item` is a string of file name.\n",
    "        This is passed in from the pandas Df\n",
    "        created by 'output_list'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        searchObj = re.search(r'20[0-9]{2}', item, re.M|re.I)\n",
    "        if searchObj:\n",
    "            year = searchObj.group()\n",
    "    except:\n",
    "        searchObj = re.search(r'19[0-9]{2}', item, re.M|re.I)\n",
    "        if searchObj:\n",
    "            year = searchObj.group()\n",
    "    return int(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "For each PDF file housed in the 'data' folder,\n",
    "either reads in a pickled output_list, or \n",
    "generates output_list. If a pickled output_list\n",
    "is not found, it will be generated. \n",
    "If OUTPUT_AS_TXT == True, reads raw PDFs into\n",
    "txt, and saves it into the 'txt' directory.\n",
    "If OUTPUT_AS_TXT == False, reads from txt files.\n",
    "If 'output_list' was generated, it will be pickled.\n",
    "The functions 'preprocess' and 'word_counter'\n",
    "is executed regardless. \n",
    "\n",
    "Create the dataframe using the list 'output_list'\n",
    "Then, apply the function 'parse_year' to \n",
    "aforementioned df['source'] and set output as df['year']\n",
    "\n",
    "\n",
    "Returns nothing, but creates df columns count, source, word, year.\n",
    "Count is count of word, source is the source of the PDF\n",
    "the word appeared in, word is the token, year is the \n",
    "year of PDF publication.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "None\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "Nothing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "LOAD_OUTPUT_LIST = True\n",
    "\n",
    "if LOAD_OUTPUT_LIST:\n",
    "    with open('model/output_list_pickled', 'rb') as f:\n",
    "        output_list = pickle.load(f)\n",
    "else:\n",
    "    OUTPUT_AS_TXT = False\n",
    "    output_list = []\n",
    "    path = \"data/\"\n",
    "    dirs = os.listdir(path)\n",
    "    for each_pdf in dirs:\n",
    "        print(each_pdf)\n",
    "        if OUTPUT_AS_TXT:\n",
    "            print(\"using raw pdf\")\n",
    "            raw = parser.from_file('data/{}'.format(each_pdf))\n",
    "            with open('txt/{}.txt'.format(str(each_pdf)), 'wb') as f:\n",
    "                f.write(raw['content'].encode(\"utf-8\"))\n",
    "            tokenized_pdf = preprocess(raw, OUTPUT_AS_TXT)\n",
    "            word_counter(tokenized = tokenized_pdf, output_list = output_list, pdf_name = each_pdf)\n",
    "        else:\n",
    "            with open('txt/{}.txt'.format(str(each_pdf)), 'r', encoding='utf8') as f:\n",
    "                text = f.read()\n",
    "                print(\"\\tpreprocessing\")\n",
    "                tokenized_pdf = preprocess(text, OUTPUT_AS_TXT)\n",
    "                print(\"\\tcounting\")\n",
    "                word_counter(tokenized = tokenized_pdf, output_list = output_list, pdf_name = each_pdf)\n",
    "    with open('model/output_list_pickled', 'wb') as f:\n",
    "        pickle.dump(output_list, f)\n",
    "\n",
    "df = pd.DataFrame(output_list)\n",
    "df['year'] = df['source'].apply(parse_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count freq of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['word'] == 'www']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total freq words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = df.groupby('word').sum()\n",
    "df_freq.sort_values(by='count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total number of words: {}\".format(df_freq['count'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df['year'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice = pd.DataFrame(columns=['count','word','year'])\n",
    "for each_year in df['year'].unique():\n",
    "    temp = df[df['year']==each_year][['count','word','year']].sort_values(by=['count'], ascending=False)[:10]\n",
    "    print(\"{}\\n\\n\".format(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total word counts by year\")\n",
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('top_words.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
